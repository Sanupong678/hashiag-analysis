"""
Data Aggregator Service
Combines data from all sources - Yahoo Finance ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡∏ü‡∏£‡∏µ, ‡πÄ‡∏£‡πá‡∏ß, ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
"""
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from db_config import db
from fetch_reddit import fetch_posts
from sentiment_analyzer import SentimentAnalyzer
from news_fetcher import NewsFetcher
from trends_fetcher import TrendsFetcher
from stock_data import StockDataFetcher
from youtube_fetcher import YouTubeFetcher
from rapidapi_fetcher import RapidAPIFetcher
from yahoo_finance_fetcher import YahooFinanceFetcher

class DataAggregator:
    def __init__(self):
        self.sentiment_analyzer = SentimentAnalyzer()
        self.yahoo_fetcher = YahooFinanceFetcher()  # ‡πÉ‡∏ä‡πâ Yahoo Finance ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
        self.news_fetcher = NewsFetcher()  # ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô backup
        self.trends_fetcher = TrendsFetcher()
        self.stock_fetcher = StockDataFetcher()
        self.youtube_fetcher = YouTubeFetcher()
        self.rapidapi_fetcher = RapidAPIFetcher()
    
    def aggregate_stock_data(self, symbol: str, days_back: int = 7) -> Dict:
        """
        Aggregate all data sources for a stock symbol
        
        Returns:
            Complete aggregated data including:
            - Stock price info
            - Reddit posts with sentiment
            - News articles with sentiment
            - Google Trends data
            - Overall sentiment score
        """
        symbol_upper = symbol.upper()
        print(f"üìä Aggregating data for {symbol_upper}...")
        
        result = {
            'symbol': symbol_upper,
            'fetchedAt': datetime.utcnow().isoformat(),
            'stockInfo': None,
            'redditData': {
                'posts': [],
                'sentiment': None,
                'mentionCount': 0
            },
            'newsData': {
                'articles': [],
                'sentiment': None,
                'articleCount': 0
            },
            'trendsData': {},
            'twitterData': {
                'tweets': [],
                'sentiment': None,
                'tweetCount': 0
            },
            'youtubeData': {
                'videos': [],
                'videoCount': 0
            },
            'overallSentiment': None
        }
        
        # 1. Fetch stock price data from Yahoo Finance (‡∏´‡∏•‡∏±‡∏Å)
        print(f"  üìà Fetching stock data from Yahoo Finance...")
        result['stockInfo'] = self.yahoo_fetcher.get_stock_info(symbol_upper)
        if not result['stockInfo']:
            # Fallback to stock_fetcher
            result['stockInfo'] = self.stock_fetcher.get_stock_info(symbol_upper)
        
        # 2. Fetch news articles from Yahoo Finance (‡∏´‡∏•‡∏±‡∏Å - ‡∏ü‡∏£‡∏µ, ‡πÄ‡∏£‡πá‡∏ß, ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
        print(f"  üì∞ Fetching news from Yahoo Finance (primary source)...")
        try:
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å Yahoo Finance ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
            yahoo_news = self.yahoo_fetcher.get_stock_news(symbol_upper, max_results=50)
            
            if yahoo_news:
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏à‡∏≤‡∏Å‡∏Ç‡πà‡∏≤‡∏ß Yahoo Finance
                texts = [f"{a.get('title', '')} {a.get('summary', '')}" for a in yahoo_news if a.get('title') or a.get('summary')]
                if texts:
                    sentiment_result = self.sentiment_analyzer.analyze_batch(texts)
                    if sentiment_result:
                        result['newsData']['sentiment'] = sentiment_result
                    result['newsData']['articles'] = yahoo_news[:30]  # Top 30
                    result['newsData']['articleCount'] = len(yahoo_news)
                    result['newsData']['source'] = 'yahoo_finance'
                    print(f"  ‚úÖ Fetched {len(yahoo_news)} news articles from Yahoo Finance")
            
            # ‡∏ñ‡πâ‡∏≤ Yahoo Finance ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ News API ‡πÄ‡∏õ‡πá‡∏ô backup
            if not yahoo_news or len(yahoo_news) < 10:
                print(f"  üì∞ Yahoo Finance has limited news, trying News API as backup...")
                try:
                    backup_news = self.news_fetcher.fetch_stock_news(symbol_upper, days_back)
                    if backup_news:
                        # ‡∏£‡∏ß‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å News API ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Yahoo Finance
                        if yahoo_news:
                            combined_news = yahoo_news + backup_news
                        else:
                            combined_news = backup_news
                        
                        texts = [f"{a.get('title', '')} {a.get('description', '')}" for a in combined_news if a.get('title') or a.get('description')]
                        if texts:
                            sentiment_result = self.sentiment_analyzer.analyze_batch(texts)
                            if sentiment_result:
                                result['newsData']['sentiment'] = sentiment_result
                            result['newsData']['articles'] = combined_news[:30]
                            result['newsData']['articleCount'] = len(combined_news)
                            result['newsData']['source'] = 'yahoo_finance+news_api'
                            print(f"  ‚úÖ Combined {len(yahoo_news) if yahoo_news else 0} Yahoo Finance + {len(backup_news)} News API articles")
                except Exception as e2:
                    print(f"  ‚ö†Ô∏è Error fetching backup news: {e2}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error fetching Yahoo Finance news: {e}")
            import traceback
            traceback.print_exc()
        
        # 3. Fetch Reddit posts (optional - ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏•‡∏á)
        print(f"  üî¥ Fetching Reddit posts (optional)...")
        try:
            reddit_posts = fetch_posts(symbol_upper, limit=50)  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 100 ‡πÄ‡∏õ‡πá‡∏ô 50
            # Also try with $ prefix
            if len(reddit_posts) < 25:
                try:
                    additional_posts = fetch_posts(f"${symbol_upper}", limit=25)  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 50 ‡πÄ‡∏õ‡πá‡∏ô 25
                    reddit_posts.extend(additional_posts)
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Error fetching Reddit posts with $ prefix: {e}")
            
            # Analyze sentiment for each post
            if reddit_posts:
                texts = [f"{p.get('title', '')} {p.get('selftext', '')}" for p in reddit_posts if p.get('title') or p.get('selftext')]
                if texts:
                    sentiment_result = self.sentiment_analyzer.analyze_batch(texts)
                    if sentiment_result:
                        result['redditData']['sentiment'] = sentiment_result
                    result['redditData']['posts'] = reddit_posts[:10]  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 20 ‡πÄ‡∏õ‡πá‡∏ô 10
                    result['redditData']['mentionCount'] = len(reddit_posts)
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error fetching Reddit data: {e}")
            import traceback
            traceback.print_exc()
        
        # 4. Fetch Google Trends
        print(f"  üìä Fetching Google Trends...")
        try:
            result['trendsData'] = self.trends_fetcher.get_stock_trends(symbol_upper)
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error fetching trends data: {e}")
        
        # 5. Fetch YouTube videos (optional)
        print(f"  üì∫ Fetching YouTube videos...")
        try:
            youtube_videos = self.youtube_fetcher.search_stock_videos(symbol_upper, max_results=10)
            if youtube_videos:
                result['youtubeData'] = {
                    'videos': youtube_videos,
                    'videoCount': len(youtube_videos),
                    'fetchedAt': datetime.utcnow().isoformat()
                }
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error fetching YouTube data: {e}")
        
        # 6. Fetch Twitter/X posts (optional)
        print(f"  üê¶ Fetching Twitter/X posts...")
        try:
            from twitter_fetcher import TwitterFetcher
            twitter_fetcher = TwitterFetcher()
            if twitter_fetcher.bearer_token:
                twitter_tweets = twitter_fetcher.track_stock_mentions(symbol_upper, max_results=50)
                if twitter_tweets:
                    texts = [t.get('text', '') for t in twitter_tweets]
                    if texts:
                        result['twitterData']['sentiment'] = self.sentiment_analyzer.analyze_batch(texts)
                        result['twitterData']['tweets'] = twitter_tweets[:20]  # Top 20
                        result['twitterData']['tweetCount'] = len(twitter_tweets)
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error fetching Twitter data: {e}")
        
        # 7. Try RapidAPI as backup for stock data (optional)
        if not result['stockInfo'] and self.rapidapi_fetcher.api_key:
            print(f"  üîÑ Trying RapidAPI as backup...")
            try:
                rapidapi_data = self.rapidapi_fetcher.fetch_stock_quote(symbol_upper)
                if rapidapi_data:
                    result['stockInfo'] = rapidapi_data
            except Exception as e:
                print(f"  ‚ö†Ô∏è Error fetching from RapidAPI: {e}")
        
        # 8. Calculate overall sentiment (weighted average)
        # Yahoo Finance ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å - ‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        print(f"  üß† Calculating overall sentiment (Yahoo Finance weighted highest)...")
        sentiment_scores = []
        weights = []
        
        # Yahoo Finance News ‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡∏´‡∏•‡∏±‡∏Å)
        if result['newsData']['sentiment'] and result['newsData'].get('source', '').startswith('yahoo'):
            sentiment_scores.append(result['newsData']['sentiment']['compound'])
            # Yahoo Finance ‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î - ‡πÉ‡∏ä‡πâ weight 1.5-2.0 ‡πÄ‡∏ó‡πà‡∏≤
            yahoo_weight = min(2.0, 1.0 + (result['newsData']['articleCount'] / 30) * 0.5)
            weights.append(yahoo_weight)
            print(f"    ‚úÖ Yahoo Finance news weight: {yahoo_weight:.2f}")
        
        # Reddit ‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î (optional)
        if result['redditData']['sentiment']:
            sentiment_scores.append(result['redditData']['sentiment']['compound'])
            # ‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å Reddit ‡∏•‡∏á‡∏°‡∏≤‡∏Å - ‡πÉ‡∏ä‡πâ weight 0.3-0.5 ‡πÄ‡∏ó‡πà‡∏≤
            reddit_weight = min(0.5, (result['redditData']['mentionCount'] / 100) * 0.3)
            weights.append(reddit_weight)
            print(f"    üî¥ Reddit weight: {reddit_weight:.2f}")
        
        # Twitter ‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡πà‡∏≥ (optional)
        if result['twitterData']['sentiment']:
            sentiment_scores.append(result['twitterData']['sentiment']['compound'])
            # Twitter ‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡πà‡∏≥ - ‡πÉ‡∏ä‡πâ weight 0.3-0.5 ‡πÄ‡∏ó‡πà‡∏≤
            twitter_weight = min(0.5, (result['twitterData']['tweetCount'] / 50) * 0.3)
            weights.append(twitter_weight)
            print(f"    üê¶ Twitter weight: {twitter_weight:.2f}")
        
        if sentiment_scores:
            total_weight = sum(weights) if weights else 1
            if total_weight > 0:
                overall_compound = sum(s * w for s, w in zip(sentiment_scores, weights)) / total_weight
            else:
                overall_compound = sum(sentiment_scores) / len(sentiment_scores)
            
            result['overallSentiment'] = {
                'compound': overall_compound,
                'label': 'positive' if overall_compound >= 0.05 else ('negative' if overall_compound <= -0.05 else 'neutral'),
                'confidence': min(1.0, total_weight)
            }
        
        # Clean result before saving to database - convert any DataFrames to dicts
        cleaned_result = self._clean_for_mongodb(result)
        
        # Save to database
        try:
            db.stock_data.update_one(
                {'symbol': symbol_upper},
                {'$set': cleaned_result},
                upsert=True
            )
            print(f"  ‚úÖ Saved to database")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error saving to database: {e}")
            import traceback
            traceback.print_exc()
        
        return cleaned_result
    
    def _clean_for_mongodb(self, data):
        """Recursively clean data structure to ensure MongoDB compatibility"""
        import pandas as pd
        
        if isinstance(data, dict):
            cleaned = {}
            for key, value in data.items():
                if isinstance(value, pd.DataFrame):
                    # Convert DataFrame to list of dicts
                    cleaned[key] = value.to_dict('records')
                elif isinstance(value, (list, tuple)):
                    cleaned[key] = [self._clean_for_mongodb(item) for item in value]
                elif isinstance(value, dict):
                    cleaned[key] = self._clean_for_mongodb(value)
                else:
                    cleaned[key] = value
            return cleaned
        elif isinstance(data, (list, tuple)):
            return [self._clean_for_mongodb(item) for item in data]
        elif isinstance(data, pd.DataFrame):
            return data.to_dict('records')
        else:
            return data
    
    def compare_stocks(self, symbols: List[str], days_back: int = 7) -> Dict:
        """Compare multiple stocks"""
        results = {}
        for symbol in symbols:
            results[symbol.upper()] = self.aggregate_stock_data(symbol, days_back)
        return results

