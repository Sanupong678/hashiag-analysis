"""
Reddit Bulk Processor - ‡∏î‡∏∂‡∏á Reddit ‡πÅ‡∏ö‡∏ö bulk (time-based)
‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á per-stock ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å
"""
import asyncio
import asyncpraw
import os
from dotenv import load_dotenv
from datetime import datetime, timedelta
from database.db_config import db
from typing import List, Dict, Optional, Set
from processors.sentiment_analyzer import SentimentAnalyzer
from utils.post_normalizer import normalize_post, get_collection_name, get_comment_collection_name
import re
import hashlib
import time

# ‡πÇ‡∏´‡∏•‡∏î .env
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '..', '..', '.env'))

class RedditBulkProcessor:
    """
    ‡∏î‡∏∂‡∏á Reddit posts ‡πÅ‡∏ö‡∏ö bulk (time-based)
    - ‡∏î‡∏∂‡∏á posts ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å 45 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (‡∏ï‡∏≤‡∏° reddit_bulk_scheduler)
    - Extract symbols ‡∏à‡∏≤‡∏Å posts
    - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡πà‡∏≠ post
    """
    
    def __init__(self):
        self.sentiment_analyzer = SentimentAnalyzer()
        # ‚úÖ ‡∏•‡∏î subreddits ‡πÄ‡∏õ‡πá‡∏ô 60 ‡∏ï‡∏±‡∏ß (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà popular ‡πÅ‡∏•‡∏∞ active ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏° popular ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏∏‡πâ‡∏ô/‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô
        self.subreddits = [
            # Tier 1: Most Popular Stock/Investing (1-25) - ‡∏ñ‡∏π‡∏Å‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
            'wallstreetbets',      # 20M+ members - Most popular
            'stocks',              # 9M+ members
            'StockMarket',         # 3.9M+ members
            'Daytrading',          # 4.9M+ members
            'pennystocks',         # 2.1M+ members
            'investing',           # 2M+ members
            'CryptoCurrency',      # 6M+ members
            'Bitcoin',             # 4M+ members
            'options',             # 1.5M+ members
            'ethereum',            # 2M+ members
            'dogecoin',            # 2M+ members
            'personalfinance',     # 15M+ members
            'financialindependence', # 1M+ members
            'RobinHood',           # 948K+ members
            'dividends',           # 400K+ members
            'algotrading',         # 200K+ members
            'ValueInvesting',      # 200K+ members
            'trading',             # 200K+ members
            'Bogleheads',          # 200K+ members
            'SecurityAnalysis',    # 150K+ members
            'investments',         # 150K+ members
            'Stock_Picks',         # 100K+ members
            'StockMarketChat',     # 80K+ members
            'SPACs',               # 70K+ members
            'weedstocks',          # 60K+ members
            
            # Tier 2: Trading & Real Estate (26-40)
            'realestateinvesting', # 200K+ members
            'realestate',          # 1M+ members
            'swingtrading',        # 40K+ members
            'Forex',               # 100K+ members
            'fatFIRE',             # 100K+ members
            'leanFIRE',            # 100K+ members
            'REBubble',            # 100K+ members
            'etfs',                # 30K+ members
            'indexfunds',          # 50K+ members
            'gold',                # 50K+ members
            'landlord',            # 50K+ members
            'FIREyFemmes',         # 50K+ members
            'forex',               # 50K+ members
            'bonds',               # 30K+ members
            'silver',              # 30K+ members
            
            # Tier 3: Technology & AI Stocks (41-55)
            'technology',          # 1M+ members
            'MachineLearning',     # 2M+ members
            'programming',         # 3M+ members
            'datascience',         # 1M+ members
            'tech',                # 500K+ members
            'artificialintelligence', # 200K+ members
            'cybersecurity',       # 100K+ members
            'artificial',          # 100K+ members
            'intel',              # 50K+ members
            'semiconductors',      # 50K+ members
            'AMD_Stock',           # 30K+ members
            'NVIDIAClub',          # 20K+ members
            'cloudcomputing',      # 20K+ members
            'chipstocks',          # 10K+ members
            
            # Tier 4: Sector-Specific & Crypto (56-60)
            'teslamotors',         # 1M+ members
            'tesla',               # 500K+ members
            'teslainvestorsclub',  # 200K+ members
            'electricvehicles',    # 200K+ members
            'automotive',          # 200K+ members
            'cardano',             # 500K+ members
            'ethtrader',           # 200K+ members
            'solana',              # 200K+ members
            'NFT',                 # 200K+ members
            'cryptomarkets'        # 100K+ members
        ]
        
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏•‡∏ö duplicates
        self.subreddits = list(dict.fromkeys(self.subreddits))  # ‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏ö duplicates
        self.last_fetched_at = None
        # ‚úÖ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ processed_post_ids ‡πÉ‡∏ô memory (‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å database ‡πÅ‡∏ó‡∏ô)
        # self.processed_post_ids: Set[str] = set()
        
        # Ticker ignore list (false positives)
        self.ignore_tickers = {
            'USD', 'GDP', 'CEO', 'IPO', 'ETF', 'SEC', 'IRS', 'FDA', 
            'AI', 'IT', 'TV', 'PC', 'USA', 'ON', 'ALL', 'FOR', 'THE',
            'AND', 'OR', 'IS', 'AT', 'TO', 'IN', 'OF', 'AS', 'BE'
        }
    
    async def get_reddit_instance(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Async Reddit instance"""
        return asyncpraw.Reddit(
            client_id=os.getenv("REDDIT_CLIENT_ID"),
            client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
            user_agent=os.getenv("USER_AGENT")
        )
    
    def extract_tickers(self, text: str, valid_tickers: Set[str]) -> Set[str]:
        """
        Extract stock tickers ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö:
        - $AAPL
        - AAPL (standalone)
        - AAPL, MSFT, TSLA (comma-separated)
        
        Args:
            text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ extract
            valid_tickers: Set ‡∏Ç‡∏≠‡∏á ticker symbols ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            
        Returns:
            Set ‡∏Ç‡∏≠‡∏á ticker symbols ‡∏ó‡∏µ‡πà‡∏û‡∏ö
        """
        if not text or not valid_tickers or len(valid_tickers) == 0:
            return set()
        
        tickers = set()
        text_upper = text.upper()
        
        # Pattern 1: $SYMBOL (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î - ‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
        dollar_pattern = re.compile(r'\$([A-Z]{1,5})\b')
        dollar_matches = dollar_pattern.findall(text_upper)
        for ticker in dollar_matches:
            if ticker in valid_tickers and ticker not in self.ignore_tickers:
                tickers.add(ticker)
        
        # Pattern 2: Standalone ticker (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ context)
        # ‡∏´‡∏≤ ticker ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÄ‡∏ä‡πà‡∏ô "buy AAPL", "AAPL stock"
        standalone_pattern = re.compile(
            r'\b(buy|sell|hold|trade|stock|shares?|ticker|symbol|NYSE|NASDAQ)\s+([A-Z]{1,5})\b',
            re.IGNORECASE
        )
        standalone_matches = standalone_pattern.findall(text_upper)
        for _, ticker in standalone_matches:
            if ticker in valid_tickers and ticker not in self.ignore_tickers:
                tickers.add(ticker)
        
        # Pattern 2b: Standalone ticker ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÄ‡∏ä‡πà‡∏ô "AAPL is", "TSLA to"
        standalone_before_pattern = re.compile(
            r'\b([A-Z]{1,5})\s+(is|to|will|can|should|going|up|down|buy|sell|hold|stock|shares?|ticker|symbol)\b',
            re.IGNORECASE
        )
        standalone_before_matches = standalone_before_pattern.findall(text_upper)
        for ticker, _ in standalone_before_matches:
            if ticker in valid_tickers and ticker not in self.ignore_tickers:
                tickers.add(ticker)
        
        # Pattern 3: Ticker ‡πÉ‡∏ô parentheses ‡∏´‡∏£‡∏∑‡∏≠ brackets
        bracket_pattern = re.compile(r'[\(\[]([A-Z]{1,5})[\)\]]')
        bracket_matches = bracket_pattern.findall(text_upper)
        for ticker in bracket_matches:
            if ticker in valid_tickers and ticker not in self.ignore_tickers:
                tickers.add(ticker)
        
        # Pattern 4: Ticker ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏±‡∏á "ticker:" ‡∏´‡∏£‡∏∑‡∏≠ "symbol:"
        colon_pattern = re.compile(r'(?:ticker|symbol):\s*([A-Z]{1,5})\b', re.IGNORECASE)
        colon_matches = colon_pattern.findall(text_upper)
        for ticker in colon_matches:
            if ticker in valid_tickers and ticker not in self.ignore_tickers:
                tickers.add(ticker)
        
        # Pattern 5: Ticker ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á spaces ‡∏´‡∏£‡∏∑‡∏≠ punctuation (‡πÄ‡∏ä‡πà‡∏ô "I like AAPL", "AAPL, MSFT")
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ context ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô ticker (‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ comma/period)
        word_boundary_pattern = re.compile(r'\b([A-Z]{2,5})\b')
        word_matches = word_boundary_pattern.findall(text_upper)
        for ticker in word_matches:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô valid ticker ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà ignore list
            if ticker in valid_tickers and ticker not in self.ignore_tickers:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö context: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏Å‡∏•‡πâ‡πÜ ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ punctuation
                ticker_pos = text_upper.find(ticker)
                if ticker_pos >= 0:
                    # ‡∏î‡∏π context ‡∏£‡∏≠‡∏ö‡πÜ ticker (50 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏á)
                    context_start = max(0, ticker_pos - 50)
                    context_end = min(len(text_upper), ticker_pos + len(ticker) + 50)
                    context = text_upper[context_start:context_end]
                    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô context ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏° ticker
                    context_keywords = ['STOCK', 'SHARE', 'TICKER', 'SYMBOL', 'BUY', 'SELL', 'HOLD', 'TRADE', 
                                     'PRICE', 'MARKET', 'INVEST', 'PORTFOLIO', 'POSITION', 'CALL', 'PUT',
                                     'OPTION', 'DIVIDEND', 'EARNINGS', 'REVENUE', 'EPS', 'PE', 'RATIO']
                    if any(keyword in context for keyword in context_keywords):
                        tickers.add(ticker)
        
        return tickers
    
    async def fetch_new_posts_bulk(self, since: Optional[datetime] = None, limit_per_subreddit: int = 500, valid_tickers: Optional[Set[str]] = None) -> List[Dict]:
        """
        ‡∏î‡∏∂‡∏á Reddit posts ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ö‡∏ö bulk
        
        Args:
            since: ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á (‡∏ñ‡πâ‡∏≤ None = ‡∏î‡∏∂‡∏á 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
            limit_per_subreddit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô posts ‡∏ï‡πà‡∏≠ subreddit (default: 500, ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á)
            valid_tickers: Set of valid ticker symbols (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö extract symbols ‡∏à‡∏≤‡∏Å comments)
            
        Returns:
            List of posts
        """
        if since is None:
            # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 1 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á posts ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô (‡πÅ‡∏°‡πâ‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏ó‡∏∏‡∏Å 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
            # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Reddit ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ posts ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
            since = datetime.utcnow() - timedelta(hours=1)
        
        all_posts = []
        reddit = await self.get_reddit_instance()
        
        from utils.progress_bar import draw_progress_bar, reset_progress
        
        print(f"   üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á posts ‡∏à‡∏≤‡∏Å {len(self.subreddits)} subreddits")
        print(f"   ‚è∞ ‡∏î‡∏∂‡∏á posts ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å: {since.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Reset progress bar
        reset_progress()
        
        # ‚úÖ ‡∏ï‡∏±‡∏ß‡∏ô‡∏±‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö comments progress bar
        total_comments_processed = 0
        comments_progress_shown = False  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÅ‡∏™‡∏î‡∏á progress bar ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö comments ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
        
        try:
            subreddit_idx = 0
            for subreddit_name in self.subreddits:
                subreddit_idx += 1
                # ‡πÅ‡∏™‡∏î‡∏á progress ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö subreddits
                draw_progress_bar(subreddit_idx, len(self.subreddits), bar_length=50, prefix="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î post_reddit", show_total=True)
                try:
                    subreddit = await reddit.subreddit(subreddit_name)
                    
                    # ‚úÖ ‡∏î‡∏∂‡∏á posts ‡πÉ‡∏´‡∏°‡πà (sort by new) - ‡πÄ‡∏û‡∏¥‡πà‡∏° limit
                    # ‡πÉ‡∏ä‡πâ limit ‡∏™‡∏π‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô (Reddit API ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ)
                    posts_count = 0
                    skipped_old = 0
                    async for submission in subreddit.new(limit=limit_per_subreddit):
                        try:
                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                            post_time = datetime.utcfromtimestamp(submission.created_utc)
                            if post_time < since:
                                skipped_old += 1
                                # ‚úÖ ‡πÉ‡∏ä‡πâ threshold ‡πÅ‡∏ö‡∏ö dynamic:
                                # - ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏°‡∏≤‡∏Å (‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏¥‡∏î‡πÑ‡∏õ‡∏ô‡∏≤‡∏ô) ‚Üí threshold ‡∏™‡∏π‡∏á (100)
                                # - ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡πÅ‡∏Ñ‡πà 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‚Üí threshold ‡∏ï‡πà‡∏≥ (20)
                                time_range = datetime.utcnow() - since
                                threshold = 100 if time_range > timedelta(hours=4) else 20
                                if skipped_old >= threshold:
                                    break
                                continue
                            
                            # ‚úÖ Reset skipped_old counter ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠ post ‡πÉ‡∏´‡∏°‡πà
                            skipped_old = 0
                            
                            # ‚úÖ ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö processed_post_ids ‡πÉ‡∏ô memory (‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å database ‡πÅ‡∏ó‡∏ô)
                            # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ processed_post_ids ‡πÉ‡∏ô memory ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å database ‡πÅ‡∏ó‡∏ô (‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏ô save_to_database)
                            
                            posts_count += 1
                            
                            # ‚úÖ ‡∏î‡∏∂‡∏á comments ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö post ‡∏ô‡∏µ‡πâ (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)
                            # Comments ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô positive ‡∏°‡∏≤‡∏Å‡πÜ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏£‡∏≤‡∏Ñ‡∏≤‡πÑ‡∏î‡πâ ‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏£‡πâ‡∏≤‡∏¢
                            # ‡∏î‡∏∂‡∏á comments ‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î 100 ‡∏ï‡∏±‡∏ß (‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á rate limit)
                            # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° delay 0.5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á comments ‡πÅ‡∏ï‡πà‡∏•‡∏∞ post ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á rate limit
                            await asyncio.sleep(0.5)
                            comments = await self.fetch_comments_for_post(submission, max_comments=100)
                            
                            # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å comments ‡∏•‡∏á database ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏à‡∏ô‡∏ñ‡∏∂‡∏á save_to_database)
                            comments_saved = False
                            if comments:
                                try:
                                    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å comments
                                    await self.save_comments_immediately(
                                        submission.id, 
                                        comments, 
                                        valid_tickers, 
                                        source='reddit',
                                        show_progress=False
                                    )
                                    comments_saved = True
                                    # ‚úÖ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï progress bar ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö comments
                                    total_comments_processed += len(comments)
                                    
                                    # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á progress bar ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö comments (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ comments ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 50)
                                    if total_comments_processed > 50 and not comments_progress_shown:
                                        comments_progress_shown = True
                                        reset_progress()  # Reset progress bar ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö comments
                                        print()  # ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà
                                    
                                    if comments_progress_shown:
                                        # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì total comments ‡∏à‡∏≤‡∏Å posts ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß + comments ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
                                        estimated_total = sum(p.get('comments_fetched', 0) for p in all_posts) + len(comments)
                                        if estimated_total > 0:
                                            draw_progress_bar(
                                                total_comments_processed, 
                                                estimated_total, 
                                                bar_length=50, 
                                                prefix="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î comment", 
                                                show_total=True
                                            )
                                except Exception:
                                    # ‡∏ñ‡πâ‡∏≤‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡πá‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£ ‡∏¢‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô post['comments'] ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á
                                    pass
                            
                            post = {
                                "id": submission.id,
                                "title": submission.title,
                                "selftext": getattr(submission, 'selftext', '') or '',
                                "score": submission.score or 0,
                                "num_comments": submission.num_comments or 0,
                                "created_utc": post_time,
                                "subreddit": str(submission.subreddit),
                                "url": submission.url,
                                "author": str(submission.author) if submission.author else "[deleted]",
                                "upvote_ratio": getattr(submission, 'upvote_ratio', 0),
                                "is_self": submission.is_self,
                                "over_18": getattr(submission, 'over_18', False),
                                "fetched_at": datetime.utcnow(),
                                "comments": comments if not comments_saved else [],  # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö comments ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory)
                                "comments_fetched": len(comments),  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô comments ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ
                                "comments_saved": comments_saved  # ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ comments ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
                            }
                            
                            all_posts.append(post)
                            # ‚úÖ ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô memory (‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å database ‡πÅ‡∏ó‡∏ô)
                            # self.processed_post_ids.add(submission.id)
                            
                            # ‚úÖ ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô posts ‡∏ï‡πà‡∏≠ subreddit ‡πÅ‡∏ö‡∏ö dynamic:
                            # - ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏°‡∏≤‡∏Å (‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏¥‡∏î‡πÑ‡∏õ‡∏ô‡∏≤‡∏ô) ‚Üí limit ‡∏™‡∏π‡∏á (2000)
                            # - ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡πÅ‡∏Ñ‡πà 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‚Üí limit ‡∏ï‡πà‡∏≥ (500)
                            time_range = datetime.utcnow() - since
                            max_posts = 2000 if time_range > timedelta(hours=4) else 500
                            if posts_count >= max_posts:
                                break
                            
                        except Exception:
                            continue
                            
                except Exception:
                    continue
            
        finally:
            await reddit.close()
        
        # ‚úÖ ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô comments ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        total_comments = sum(post.get('comments_fetched', 0) for post in all_posts)
        
        # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á progress bar ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö posts
        draw_progress_bar(len(self.subreddits), len(self.subreddits), bar_length=50, prefix="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î post_reddit", show_total=True)
        
        # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á progress bar ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö comments (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ comments ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á progress bar ‡πÅ‡∏•‡πâ‡∏ß)
        if total_comments > 0 and comments_progress_shown:
            # ‡πÅ‡∏™‡∏î‡∏á progress bar ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
            draw_progress_bar(total_comments, total_comments, bar_length=50, prefix="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î comment", show_total=True)
        
        if len(all_posts) == 0:
            print(f"   üí° Tip: ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ posts ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡∏´‡∏£‡∏∑‡∏≠ Reddit API rate limit")
        
        return all_posts
    
    async def fetch_comments_for_post(self, submission, max_comments: int = 100) -> List[Dict]:
        """
        ‡∏î‡∏∂‡∏á comments ‡∏à‡∏≤‡∏Å Reddit post (‡πÑ‡∏°‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏ï‡∏≠‡∏ô‡∏î‡∏∂‡∏á - ‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á)
        
        Args:
            submission: Reddit submission object
            max_comments: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô comments ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á (default: 100)
            
        Returns:
            List of comments (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment)
        """
        comments = []
        try:
            # ‚úÖ ‡∏ï‡πâ‡∏≠‡∏á load submission ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á comments
            await submission.load()
            
            # ‚úÖ ‡∏î‡∏∂‡∏á comments (asyncpraw ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ await)
            comments_forest = await submission.comments()
            
            # ‚úÖ Replace MoreComments instances (limit=None = ‡∏î‡∏∂‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
            await comments_forest.replace_more(limit=None)
            
            # ‚úÖ Get flattened list of comments
            # ‡πÉ‡∏ô asyncpraw, .list() ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô list object ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà awaitable
            # ‡πÉ‡∏ä‡πâ list() ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà await
            all_comments = comments_forest.list()
            
            comment_count = 0
            for comment in all_comments:
                if comment_count >= max_comments:
                    break
                
                # ‡∏Ç‡πâ‡∏≤‡∏° deleted/removed comments
                if not hasattr(comment, 'body') or comment.body in ['[deleted]', '[removed]']:
                    continue
                
                # ‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• comment ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment (‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á)
                comment_text = comment.body or ''
                if not comment_text.strip():
                    continue
                
                # ‚úÖ ‡πÑ‡∏°‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ - ‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á database
                comment_data = {
                    "id": comment.id,
                    "body": comment_text,  # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ß‡πâ
                    "score": comment.score or 0,
                    "author": str(comment.author) if comment.author else "[deleted]",
                    "created_utc": datetime.utcfromtimestamp(comment.created_utc) if hasattr(comment, 'created_utc') else datetime.utcnow(),
                    "sentiment": None,  # ‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á
                    "is_submitter": comment.is_submitter if hasattr(comment, 'is_submitter') else False,
                    "parent_id": str(comment.parent_id) if hasattr(comment, 'parent_id') else None
                }
                
                comments.append(comment_data)
                comment_count += 1
                
        except Exception as e:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á comments ‚Üí ‡∏Ç‡πâ‡∏≤‡∏° (‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á posts)
            error_msg = str(e)
            # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô rate limit error ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if "429" in error_msg or "rate limit" in error_msg.lower():
                print(f"   ‚ö†Ô∏è  Rate limit hit for post {submission.id if hasattr(submission, 'id') else 'unknown'}: skipping comments")
            else:
                print(f"   ‚ö†Ô∏è  Error fetching comments for post {submission.id if hasattr(submission, 'id') else 'unknown'}: {error_msg}")
        
        return comments
    
    async def save_comments_immediately(self, post_id: str, comments: List[Dict], valid_tickers: Optional[Set[str]] = None, source: str = 'reddit', show_progress: bool = False):
        """
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å comments ‡∏•‡∏á database ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏î‡∏∂‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏à‡∏ô‡∏ñ‡∏∂‡∏á save_to_database)
        
        Args:
            post_id: Post ID
            comments: List of comments ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å Reddit
            valid_tickers: Set of valid ticker symbols
            source: Source platform ('reddit', 'yahoo', 'x', 'youtube', etc.)
            show_progress: ‡πÅ‡∏™‡∏î‡∏á progress bar ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (default: False ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)
        """
        if not comments:
            return
        
        if db is None:
            return
        
        from utils.post_normalizer import get_comment_collection_name
        from utils.progress_bar import draw_progress_bar
        comment_collection_name = get_comment_collection_name(source)
        
        # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á comment collection ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
        if comment_collection_name not in db.list_collection_names():
            try:
                db.create_collection(comment_collection_name)
                db[comment_collection_name].create_index("id", unique=True)
                db[comment_collection_name].create_index("post_id")
                db[comment_collection_name].create_index("created_utc")
                db[comment_collection_name].create_index("author")
                db[comment_collection_name].create_index("symbols")
                db[comment_collection_name].create_index([("post_id", 1), ("created_utc", -1)])
            except Exception as e:
                return
        
        comment_collection = getattr(db, comment_collection_name)
        if comment_collection is None:
            return
        
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö comment IDs ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
        try:
            recent_comments = list(comment_collection.find(
                {"post_id": post_id},
                {"id": 1},
                limit=1000
            ))
            existing_comment_ids = {c.get("id") for c in recent_comments if c.get("id")}
        except Exception:
            existing_comment_ids = set()
        
        # ‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° normalized comments
        normalized_comments = []
        skipped_count = 0
        total_comments = len(comments)
        
        # ‡πÅ‡∏™‡∏î‡∏á progress bar ‡∏ñ‡πâ‡∏≤ show_progress = True
        if show_progress and total_comments > 10:
            draw_progress_bar(0, total_comments, bar_length=50, prefix="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î comment", show_total=True)
        
        for idx, comment in enumerate(comments):
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï progress bar
            if show_progress and total_comments > 10:
                draw_progress_bar(idx, total_comments, bar_length=50, prefix="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î comment", show_total=True)
            
            comment_id = comment.get('id')
            if not comment_id:
                skipped_count += 1
                continue
            if comment_id in existing_comment_ids:
                skipped_count += 1
                continue
            
            # Extract symbols ‡∏à‡∏≤‡∏Å comment body
            comment_body = comment.get('body', '') or ''
            
            # Extract symbols
            if valid_tickers and comment_body.strip():
                comment_symbols = self.extract_tickers(comment_body, valid_tickers)
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ valid_tickers ‡∏´‡∏£‡∏∑‡∏≠ body ‡∏ß‡πà‡∏≤‡∏á ‚Üí extract ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢
                if comment_body.strip():
                    dollar_pattern = re.compile(r'\$([A-Z]{1,5})\b')
                    comment_symbols = set(dollar_pattern.findall(comment_body.upper()))
                else:
                    comment_symbols = set()
            
            # ‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment
            comment_sentiment = {}
            if comment_body.strip():
                try:
                    comment_sentiment = self.sentiment_analyzer.analyze(comment_body)
                except Exception:
                    comment_sentiment = {"compound": 0.0, "pos": 0.0, "neu": 1.0, "neg": 0.0}
            else:
                # ‡∏ñ‡πâ‡∏≤ body ‡∏ß‡πà‡∏≤‡∏á ‚Üí neutral sentiment
                comment_sentiment = {"compound": 0.0, "pos": 0.0, "neu": 1.0, "neg": 0.0}
            
            normalized_comment = {
                "id": comment_id,
                "post_id": post_id,
                "body": comment_body,
                "score": comment.get('score', 0),
                "author": comment.get('author', '[deleted]'),
                "created_utc": comment.get('created_utc'),
                "sentiment": comment_sentiment,
                "is_submitter": comment.get('is_submitter', False),
                "parent_id": comment.get('parent_id'),
                "fetched_at": datetime.utcnow(),
                "symbols": list(comment_symbols) if comment_symbols else [],
                "platform": source
            }
            normalized_comments.append(normalized_comment)
            existing_comment_ids.add(comment_id)  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô duplicates
        
        # ‚úÖ Bulk insert comments
        if normalized_comments:
            try:
                comment_collection.insert_many(normalized_comments, ordered=False)
            except Exception:
                # ‡∏ñ‡πâ‡∏≤ insert_many ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‚Üí insert ‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß
                saved_count = 0
                for comment in normalized_comments:
                    try:
                        comment_collection.update_one(
                            {"id": comment['id']},
                            {"$set": comment},
                            upsert=True
                        )
                        saved_count += 1
                    except Exception:
                        continue
    
    def calculate_combined_sentiment(self, post_sentiment: Dict, comments: List[Dict]) -> Dict:
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì sentiment ‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å post + comments (weighted by upvotes)
        
        Args:
            post_sentiment: Sentiment ‡∏Ç‡∏≠‡∏á post
            comments: List of comments with sentiment
            
        Returns:
            Combined sentiment dictionary
        """
        if not comments:
            return post_sentiment
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì weighted average ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ upvotes ‡πÄ‡∏õ‡πá‡∏ô weight
        # Post ‡∏°‡∏µ weight = 1.0 (base weight)
        # Comments ‡∏°‡∏µ weight = log(score + 1) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î impact ‡∏Ç‡∏≠‡∏á comments ‡∏ó‡∏µ‡πà‡∏°‡∏µ upvotes ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å
        
        import math
        
        # Post weight
        post_weight = 1.0
        post_compound = post_sentiment.get('compound', 0.0)
        
        # Comments weights ‡πÅ‡∏•‡∏∞ sentiments
        total_weight = post_weight
        weighted_compound = post_compound * post_weight
        
        comment_sentiments = []
        for comment in comments:
            comment_sentiment = comment.get('sentiment', {})
            comment_compound = comment_sentiment.get('compound', 0.0)
            comment_score = comment.get('score', 0)
            
            # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ comment_score + 1 > 0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô math domain error
            # ‡∏ñ‡πâ‡∏≤ comment_score ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏•‡∏ö‡∏°‡∏≤‡∏Å (‡πÄ‡∏ä‡πà‡∏ô -10) ‚Üí comment_score + 1 ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏•‡∏ö
            score_for_log = max(1, comment_score + 1)  # ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ >= 1
            
            # Weight = log(score + 1) + 0.1 (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ comments ‡∏ó‡∏µ‡πà‡∏°‡∏µ upvotes ‡∏™‡∏π‡∏á‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤)
            # ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (log scale)
            comment_weight = math.log(score_for_log) + 0.1
            
            weighted_compound += comment_compound * comment_weight
            total_weight += comment_weight
            
            comment_sentiments.append(comment_compound)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì average
        if total_weight > 0:
            combined_compound = weighted_compound / total_weight
        else:
            combined_compound = post_compound
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì positive/negative/neutral ‡∏à‡∏≤‡∏Å combined compound
        if combined_compound >= 0.05:
            label = "positive"
            positive = min(1.0, combined_compound)
            negative = 0.0
        elif combined_compound <= -0.05:
            label = "negative"
            positive = 0.0
            negative = min(1.0, abs(combined_compound))
        else:
            label = "neutral"
            positive = 0.0
            negative = 0.0
        
        neutral = 1.0 - positive - negative
        
        return {
            "compound": combined_compound,
            "positive": positive,
            "negative": negative,
            "neutral": neutral,
            "label": label,
            "post_sentiment": post_sentiment,  # ‡πÄ‡∏Å‡πá‡∏ö original post sentiment
            "comments_count": len(comments),
            "comments_avg_sentiment": sum(comment_sentiments) / len(comment_sentiments) if comment_sentiments else 0.0
        }
    
    def analyze_post_sentiment(self, post: Dict) -> Dict:
        """
        ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏ï‡πà‡∏≠ post (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
        
        Args:
            post: Post dictionary
            
        Returns:
            Sentiment dictionary
        """
        text = f"{post.get('title', '')} {post.get('selftext', '')}".strip()
        if not text:
            return {
                "compound": 0.0,
                "positive": 0.0,
                "negative": 0.0,
                "neutral": 1.0,
                "label": "neutral"
            }
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
        sentiment = self.sentiment_analyzer.analyze(text)
        
        return sentiment
    
    async def process_bulk_posts(self, posts: List[Dict], valid_tickers: Set[str]) -> Dict[str, List[Dict]]:
        """
        Process posts ‡πÅ‡∏ö‡∏ö bulk:
        1. Extract tickers
        2. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡πà‡∏≠ post)
        3. Group by symbol
        
        Args:
            posts: List of posts
            valid_tickers: Set of valid ticker symbols
            
        Returns:
            Dictionary {symbol: [posts]}
        """
        symbol_posts = {}  # {symbol: [posts]}
        processed_posts = []  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á database
        
        posts_without_tickers = 0
        for post in posts:
            # Extract tickers (‡∏£‡∏ß‡∏° comments ‡∏î‡πâ‡∏ß‡∏¢ - comments ‡∏≠‡∏≤‡∏à‡∏°‡∏µ tickers!)
            text = f"{post.get('title', '')} {post.get('selftext', '')}"
            # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° comments text ‡πÄ‡∏û‡∏∑‡πà‡∏≠ extract tickers ‡∏à‡∏≤‡∏Å comments ‡∏î‡πâ‡∏ß‡∏¢
            comments = post.get('comments', [])
            if comments:
                comments_text = ' '.join([c.get('body', '') for c in comments])
                text += ' ' + comments_text
            symbols = self.extract_tickers(text, valid_tickers)
            
            # ‚úÖ Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á extraction (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 3 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
            if posts_without_tickers < 3 and not symbols:
                sample_text = text[:150]
                print(f"   üîç Debug post #{posts_without_tickers + 1} (‡πÑ‡∏°‡πà‡∏°‡∏µ ticker): {sample_text}...")
            
            if not symbols:
                posts_without_tickers += 1
                continue  # ‡∏Ç‡πâ‡∏≤‡∏° post ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ ticker
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏Ç‡∏≠‡∏á post
            post_sentiment = self.analyze_post_sentiment(post)
            
            # ‚úÖ ‡πÉ‡∏ä‡πâ post sentiment ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° comments ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ comments ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment)
            # Comments ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏ï‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á database
            # ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì combined sentiment ‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ
            post['sentiment'] = post_sentiment
            post['post_sentiment'] = post_sentiment  # ‡πÄ‡∏Å‡πá‡∏ö original post sentiment ‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏ß‡∏¢
            
            # Group by symbol
            for symbol in symbols:
                if symbol not in symbol_posts:
                    symbol_posts[symbol] = []
                symbol_posts[symbol].append(post)
            
            # ‡πÄ‡∏Å‡πá‡∏ö post ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á database
            processed_posts.append({
                **post,
                "symbols": list(symbols),
                "sentiment": post.get('sentiment')  # ‡πÉ‡∏ä‡πâ combined sentiment (post + comments)
            })
        
        # ‚úÖ Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô posts ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ ticker ‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ comments
        if posts_without_tickers > 0:
            print(f"   ‚ö†Ô∏è  Posts ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ ticker: {posts_without_tickers}/{len(posts)}")
        
        # ‚úÖ ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ comments
        total_comments_in_posts = sum(len(post.get('comments', [])) for post in processed_posts)
        posts_with_comments = sum(1 for post in processed_posts if post.get('comments'))
        if total_comments_in_posts > 0:
            print(f"   üí¨ Comments: {total_comments_in_posts} comments ‡∏à‡∏≤‡∏Å {posts_with_comments} posts (‡∏£‡∏ß‡∏° sentiment ‡∏à‡∏≤‡∏Å comments ‡πÅ‡∏•‡πâ‡∏ß!)")
        
        return {
            "symbol_posts": symbol_posts,
            "processed_posts": processed_posts
        }
    
    async def save_to_database(self, processed_posts: List[Dict], valid_tickers: Optional[Set[str]] = None, source: str = 'reddit'):
        """
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å posts ‡πÅ‡∏•‡∏∞ comments ‡∏•‡∏á database ‡πÅ‡∏ö‡∏ö bulk
        Comments ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏¢‡∏Å‡πÉ‡∏ô comment collection ‡∏ï‡∏≤‡∏° platform (comment_reddit, comment_yahoo, etc.)
        
        Args:
            processed_posts: List of processed posts
            valid_tickers: Set of valid ticker symbols (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö extract symbols ‡∏à‡∏≤‡∏Å comments)
            source: Source platform ('reddit', 'yahoo', 'x', 'youtube', etc.)
        """
        if not processed_posts:
            return
        
        collection_name = get_collection_name(source)
        comment_collection_name = get_comment_collection_name(source)
        
        if db is None:
            return
        
        if not hasattr(db, collection_name):
            return
        
        post_collection = getattr(db, collection_name)
        comment_collection = None
        
        # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á comment collection ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
        if comment_collection_name not in db.list_collection_names():
            try:
                db.create_collection(comment_collection_name)
                print(f"   ‚úÖ Created {comment_collection_name} collection")
                # Create indexes
                db[comment_collection_name].create_index("id", unique=True)
                db[comment_collection_name].create_index("post_id")
                db[comment_collection_name].create_index("created_utc")
                db[comment_collection_name].create_index("author")
                db[comment_collection_name].create_index("symbols")
                db[comment_collection_name].create_index([("post_id", 1), ("created_utc", -1)])
                print(f"   ‚úÖ Created indexes for {comment_collection_name} collection")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error creating comment collection: {e}")
                import traceback
                traceback.print_exc()
        
        # ‚úÖ ‡πÉ‡∏ä‡πâ list_collection_names() ‡πÅ‡∏ó‡∏ô hasattr() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö collection
        if comment_collection_name in db.list_collection_names():
            comment_collection = getattr(db, comment_collection_name)
        else:
            print(f"   ‚ö†Ô∏è  {comment_collection_name} collection not found, comments will not be saved")
            comment_collection = None
        
        # Normalize ‡πÅ‡∏•‡∏∞ prepare posts
        normalized_posts = []
        normalized_comments = []
        
        # ‚úÖ ‡πÉ‡∏ä‡πâ distinct ‡πÅ‡∏ö‡∏ö‡∏°‡∏µ limit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 10000 posts ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
        try:
            recent_posts = list(post_collection.find(
                {},
                {"id": 1},
                sort=[("created_utc", -1)],
                limit=10000
            ))
            existing_post_ids = {p.get("id") for p in recent_posts if p.get("id")}
        except Exception:
            existing_post_ids = set(post_collection.distinct("id"))
        
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö comment IDs ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á duplicates)
        existing_comment_ids = set()
        if comment_collection:
            try:
                recent_comments = list(comment_collection.find(
                    {},
                    {"id": 1},
                    sort=[("created_utc", -1)],
                    limit=50000  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö 50k comments ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                ))
                existing_comment_ids = {c.get("id") for c in recent_comments if c.get("id")}
            except Exception:
                pass
        
        for post in processed_posts:
            post_id = post['id']
            
            # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ comments ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
            comments_saved = post.get('comments_saved', False)
            comments = post.get('comments', [])
            comments_count = post.get('comments_fetched', len(comments))
            
            # ‚úÖ ‡∏ñ‡πâ‡∏≤ comments ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß ‚Üí ‡∏Ç‡πâ‡∏≤‡∏° (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞ resources)
            if not comments_saved and comments:
                # ‚úÖ Extract symbols ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏à‡∏≤‡∏Å comments (‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å)
                # ‚úÖ Debug: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ comments ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                if len(normalized_comments) < 3:
                    print(f"   üîç Debug: Post {post_id} ‡∏°‡∏µ {comments_count} comments (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å)")
                
                # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å comments ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
                for comment in comments:
                    comment_id = comment.get('id')
                    if not comment_id or comment_id in existing_comment_ids:
                        continue
                    
                    # Extract symbols ‡∏à‡∏≤‡∏Å comment body
                    comment_body = comment.get('body', '')
                    if valid_tickers:
                        comment_symbols = self.extract_tickers(comment_body, valid_tickers)
                    else:
                        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ valid_tickers ‚Üí extract ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ (‡∏´‡∏≤ $SYMBOL pattern)
                        dollar_pattern = re.compile(r'\$([A-Z]{1,5})\b')
                        comment_symbols = set(dollar_pattern.findall(comment_body.upper()))
                    
                    # ‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏ï‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á database (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏≠‡∏ô‡∏î‡∏∂‡∏á)
                    comment_sentiment = {}
                    if comment_body.strip():
                        comment_sentiment = self.sentiment_analyzer.analyze(comment_body)
                    
                    normalized_comment = {
                        "id": comment_id,
                        "post_id": post_id,
                        "body": comment_body,  # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ß‡πâ
                        "score": comment.get('score', 0),
                        "author": comment.get('author', '[deleted]'),
                        "created_utc": comment.get('created_utc'),
                        "sentiment": comment_sentiment,  # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
                        "is_submitter": comment.get('is_submitter', False),
                        "parent_id": comment.get('parent_id'),
                        "fetched_at": datetime.utcnow(),
                        "symbols": list(comment_symbols) if comment_symbols else [],
                        "platform": source  # ‡πÄ‡∏û‡∏¥‡πà‡∏° platform ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏ source
                    }
                    normalized_comments.append(normalized_comment)
                    existing_comment_ids.add(comment_id)  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô duplicates ‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                
                # ‚úÖ Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô comments ‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
                if len(normalized_comments) > 0 and len(normalized_comments) % 100 == 0:
                    print(f"   üîç Debug: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° {len(normalized_comments)} comments ‡πÅ‡∏•‡πâ‡∏ß...")
            
            # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ post ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
            if post_id in existing_post_ids:
                continue
            
            # Normalize post (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö comments array)
            first_symbol = post.get('symbols', [])[0] if post.get('symbols') else ''
            normalized = normalize_post(post, 'reddit', first_symbol)
            
            # ‚úÖ ‡πÄ‡∏≠‡∏≤ comments array ‡∏≠‡∏≠‡∏Å ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà comments_count
            if 'comments' in normalized:
                del normalized['comments']
            normalized['comments_count'] = comments_count  # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà‡∏à‡∏≥‡∏ô‡∏ß‡∏ô
            normalized['comments_fetched'] = comments_count  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö backward compatibility
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° symbols array
            normalized['symbols'] = post.get('symbols', [])
            if first_symbol:
                normalized['keyword'] = first_symbol
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° sentiment
            normalized['sentiment'] = post.get('sentiment', {})
            normalized_posts.append(normalized)
        
        # ‚úÖ Bulk insert posts
        if normalized_posts:
            try:
                post_collection.insert_many(normalized_posts, ordered=False)
            except Exception:
                # ‡∏ñ‡πâ‡∏≤ insert_many ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‚Üí insert ‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß
                saved_posts = 0
                for post in normalized_posts:
                    try:
                        post_collection.update_one(
                            {"id": post['id']},
                            {"$set": post},
                            upsert=True
                        )
                        saved_posts += 1
                    except Exception:
                        continue
        
        # ‚úÖ Bulk insert comments
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ collection ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if comment_collection_name not in db.list_collection_names():
            try:
                db.create_collection(comment_collection_name)
                db[comment_collection_name].create_index("id", unique=True)
                db[comment_collection_name].create_index("post_id")
                db[comment_collection_name].create_index("created_utc")
                db[comment_collection_name].create_index("author")
                db[comment_collection_name].create_index("symbols")
                db[comment_collection_name].create_index([("post_id", 1), ("created_utc", -1)])
                comment_collection = getattr(db, comment_collection_name)
            except Exception:
                comment_collection = None
        
        if normalized_comments and comment_collection:
            try:
                comment_collection.insert_many(normalized_comments, ordered=False)
            except Exception:
                # ‡∏ñ‡πâ‡∏≤ insert_many ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‚Üí insert ‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß
                saved_comments = 0
                for comment in normalized_comments:
                    try:
                        comment_collection.update_one(
                            {"id": comment['id']},
                            {"$set": comment},
                            upsert=True
                        )
                        saved_comments += 1
                    except Exception:
                        continue
    
    async def run_bulk_fetch(self, valid_tickers: Optional[Set[str]] = None) -> Dict:
        """
        ‡∏£‡∏±‡∏ô bulk fetch process
        
        Args:
            valid_tickers: Set of valid ticker symbols (‡∏ñ‡πâ‡∏≤ None ‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å database)
            
        Returns:
            Dictionary with results
        """
        # ‡∏î‡∏∂‡∏á valid tickers
        if valid_tickers is None:
            from utils.stock_list_fetcher import stock_list_fetcher
            all_tickers = stock_list_fetcher.get_all_valid_tickers(force_refresh=False)
            valid_tickers = {t.upper() for t in all_tickers}
        
        # ‡∏î‡∏∂‡∏á last_fetched_at ‡∏à‡∏≤‡∏Å database
        collection_name = get_collection_name('reddit')
        if db is not None and hasattr(db, collection_name):
            post_collection = getattr(db, collection_name)
            latest_post = post_collection.find_one(sort=[("created_utc", -1)])
            if latest_post:
                latest_date = latest_post.get('created_utc')
                if isinstance(latest_date, str):
                    self.last_fetched_at = datetime.fromisoformat(latest_date.replace('Z', '+00:00'))
                elif isinstance(latest_date, datetime):
                    self.last_fetched_at = latest_date
                else:
                    # ‚úÖ ‡∏ñ‡πâ‡∏≤ latest_date ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà string ‡∏´‡∏£‡∏∑‡∏≠ datetime ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
                    self.last_fetched_at = datetime.utcnow() - timedelta(hours=2)
            else:
                # ‚úÖ ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ posts ‡πÉ‡∏ô database ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà 7 ‡∏ß‡∏±‡∏ô)
                # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏î‡∏∂‡∏á‡∏ó‡∏∏‡∏Å 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏°‡∏≤‡∏Å
                self.last_fetched_at = datetime.utcnow() - timedelta(hours=2)
        else:
            # ‚úÖ ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ database ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà 7 ‡∏ß‡∏±‡∏ô)
            self.last_fetched_at = datetime.utcnow() - timedelta(hours=2)
        
        # ‚úÖ ‡∏î‡∏∂‡∏á posts ‡πÉ‡∏´‡∏°‡πà - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ last_fetched_at ‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏¥‡∏î‡πÑ‡∏õ) ‚Üí ‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏¥‡∏î‡πÑ‡∏õ
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πà‡∏≤ (‡∏î‡∏∂‡∏á‡∏ó‡∏∏‡∏Å 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ) ‚Üí ‡∏î‡∏∂‡∏á‡πÅ‡∏Ñ‡πà posts ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏°‡πà‡∏Å‡∏ß‡πà‡∏≤ last_fetched_at
        now = datetime.utcnow()
        if self.last_fetched_at:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ last_fetched_at ‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô
            time_since_last_fetch = now - self.last_fetched_at
            
            if time_since_last_fetch > timedelta(hours=2):
                # ‚úÖ ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏¥‡∏î‡πÑ‡∏õ‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤ 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á ‚Üí ‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏¥‡∏î‡πÑ‡∏õ
                # ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å last_fetched_at (‡πÄ‡∏û‡∏¥‡πà‡∏° buffer 5 ‡∏ô‡∏≤‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏•‡∏≤‡∏î)
                since_time = self.last_fetched_at - timedelta(minutes=5)
                print(f"   ‚ö†Ô∏è  ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏¥‡∏î‡πÑ‡∏õ {int(time_since_last_fetch.total_seconds() / 3600)} ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á ‚Üí ‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å {since_time.strftime('%Y-%m-%d %H:%M:%S')}")
            else:
                # ‚úÖ ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥ (‡∏î‡∏∂‡∏á‡∏ó‡∏∏‡∏Å 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ) ‚Üí ‡∏î‡∏∂‡∏á‡πÅ‡∏Ñ‡πà posts ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏°‡πà‡∏Å‡∏ß‡πà‡∏≤ last_fetched_at
                since_time = self.last_fetched_at - timedelta(minutes=5)
        else:
            # ‚úÖ ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ last_fetched_at) ‚Üí ‡∏î‡∏∂‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
            since_time = now - timedelta(hours=2)
        posts = await self.fetch_new_posts_bulk(since=since_time, valid_tickers=valid_tickers)
        
        if not posts:
            return {
                "posts_fetched": 0,
                "posts_processed": 0,
                "symbols_found": 0,
                "posts_saved": 0
            }
        
        # ‚úÖ Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• valid_tickers
        print(f"   üîç Valid tickers count: {len(valid_tickers)}")
        if len(valid_tickers) == 0:
            print(f"   ‚ö†Ô∏è  WARNING: No valid tickers found! Posts will not be saved.")
        
        # Process posts
        result = await self.process_bulk_posts(posts, valid_tickers)
        symbol_posts = result["symbol_posts"]
        processed_posts = result["processed_posts"]
        
        # ‚úÖ Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á posts ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ ticker (‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ posts ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ ticker)
        if len(posts) > 0:
            if len(processed_posts) == 0:
                print(f"   ‚ö†Ô∏è  Posts ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ ticker: {len(posts)} posts")
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á posts ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
                for i, sample_post in enumerate(posts[:5]):  # ‡πÅ‡∏™‡∏î‡∏á 5 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
                    sample_text = f"{sample_post.get('title', '')} {sample_post.get('selftext', '')}"[:300]
                    print(f"   üìù ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á post #{i+1}: {sample_text}...")
                    # ‡∏•‡∏≠‡∏á extract tickers ‡πÅ‡∏ö‡∏ö debug
                    debug_symbols = self.extract_tickers(sample_text, valid_tickers)
                    print(f"   üîç Tickers ‡∏ó‡∏µ‡πà extract ‡πÑ‡∏î‡πâ: {debug_symbols}")
                if len(valid_tickers) > 0:
                    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á valid tickers
                    sample_tickers = list(valid_tickers)[:20]  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 20 ‡∏ï‡∏±‡∏ß
                    print(f"   üìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á valid tickers (20 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å): {sample_tickers}")
            else:
                # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á posts ‡∏ó‡∏µ‡πà‡∏°‡∏µ ticker (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)
                print(f"   ‚úÖ Posts ‡∏ó‡∏µ‡πà‡∏°‡∏µ ticker: {len(processed_posts)}/{len(posts)}")
                if len(processed_posts) > 0:
                    sample_processed = processed_posts[0]
                    sample_symbols = sample_processed.get('symbols', [])
                    sample_title = sample_processed.get('title', '')[:100]
                    print(f"   üìù ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á post ‡∏ó‡∏µ‡πà‡∏°‡∏µ ticker: {sample_title}... ‚Üí Symbols: {sample_symbols}")
        
        # ‚úÖ Debug: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö comments ‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        total_comments_in_processed = sum(len(post.get('comments', [])) for post in processed_posts)
        print(f"   üîç Debug: processed_posts = {len(processed_posts)}, total comments in posts = {total_comments_in_processed}")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á database (‡πÉ‡∏ä‡πâ source='reddit' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Reddit bulk processor)
        await self.save_to_database(processed_posts, valid_tickers, source='reddit')
        
        # ‚úÖ ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô comments ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏î‡πâ
        from utils.post_normalizer import get_comment_collection_name
        comment_collection_name = get_comment_collection_name('reddit')
        total_comments_saved = 0
        if db is not None and comment_collection_name in db.list_collection_names():
            comment_collection = getattr(db, comment_collection_name)
            if comment_collection:
                # ‡∏ô‡∏±‡∏ö comments ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏à‡∏≤‡∏Å posts ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å)
                try:
                    # ‡∏ô‡∏±‡∏ö comments ‡∏à‡∏≤‡∏Å posts ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡πÉ‡∏ä‡πâ post_id ‡∏à‡∏≤‡∏Å processed_posts)
                    post_ids = [p.get('id') for p in processed_posts if p.get('id')]
                    if post_ids:
                        total_comments_saved = comment_collection.count_documents({"post_id": {"$in": post_ids}})
                except Exception:
                    pass
        
        # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
        print(f"\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤:")
        print(f"   üìù Reddit Posts: {len(processed_posts):,} posts")
        print(f"   üí¨ Comments: {total_comments_saved:,} comments")
        print(f"   üè∑Ô∏è  Symbols: {len(symbol_posts):,} symbols")
        
        return {
            "posts_fetched": len(posts),
            "posts_processed": len(processed_posts),
            "symbols_found": len(symbol_posts),
            "posts_saved": len(processed_posts),
            "comments_saved": total_comments_saved,
            "symbol_posts": symbol_posts
        }
