"""
Batch Data Processor - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏ö‡∏ö batch
‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á real-time ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà request
"""
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set
from database.db_config import db
from processors.data_aggregator import DataAggregator
from fetchers.yahoo_finance_fetcher import YahooFinanceFetcher
from processors.sentiment_analyzer import SentimentAnalyzer
from processors.async_stock_fetcher import AsyncStockFetcher
from processors.enhanced_sentiment_aggregator import EnhancedSentimentAggregator
# ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ Redis cache - ‡∏•‡∏î memory usage
# from cache.redis_cache import cache
cache = None
import asyncio
import logging
import warnings

# Suppress aiohttp warnings (Unclosed client session)
warnings.filterwarnings('ignore', category=ResourceWarning)
logging.getLogger('aiohttp').setLevel(logging.ERROR)
logging.getLogger('aiohttp.client').setLevel(logging.ERROR)
logging.getLogger('aiohttp.connector').setLevel(logging.ERROR)
import hashlib
import time

class BatchDataProcessor:
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏ö‡∏ö batch
    - ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏≤‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    - ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô database
    - ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ó‡∏∏‡∏Å 1-2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
    - ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥ (deduplication)
    """
    
    def __init__(self, days_back: int = 7, update_interval_hours: int = 2):
        """
        Args:
            days_back: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (default: 7 ‡∏ß‡∏±‡∏ô)
            update_interval_hours: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏Å‡∏µ‡πà‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (default: 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)
        """
        self.days_back = days_back
        self.update_interval_hours = update_interval_hours
        self.skip_reddit = False  # ‡∏Ç‡πâ‡∏≤‡∏° Reddit ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        self.reddit_from_db_only = False  # ‡πÉ‡∏ä‡πâ Reddit ‡∏à‡∏≤‡∏Å database ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        self.reddit_limit = 500  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Reddit posts ‡∏ï‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô
        self.reddit_priority_only = False  # ‡∏î‡∏∂‡∏á Reddit ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏∏‡πâ‡∏ô popular
        self.reddit_incremental = False  # ‡∏î‡∏∂‡∏á Reddit ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ/‡πÄ‡∏Å‡πà‡∏≤
        self.data_aggregator = DataAggregator()
        self.yahoo_fetcher = YahooFinanceFetcher()
        self.sentiment_analyzer = SentimentAnalyzer()
        self.enhanced_sentiment_aggregator = EnhancedSentimentAggregator()
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß: max_concurrent=2000, rate_limit=2000 (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° 4 ‡πÄ‡∏ó‡πà‡∏≤)
        # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ Redis cache - ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        self.async_fetcher = AsyncStockFetcher(max_concurrent=2000, rate_limit=2000)
        
        # ‡πÄ‡∏Å‡πá‡∏ö hash ‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥)
        self.processed_news_hashes: Set[str] = set()
    
    def _generate_news_hash(self, article: Dict) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á hash ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ã‡πâ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        
        Args:
            article: ‡∏Ç‡πà‡∏≤‡∏ß article dict
        
        Returns:
            MD5 hash string
        """
        # ‡πÉ‡∏ä‡πâ title + url + publishedAt ‡πÄ‡∏õ‡πá‡∏ô unique identifier
        unique_string = f"{article.get('title', '')}{article.get('url', '')}{article.get('publishedAt', '')}"
        return hashlib.md5(unique_string.encode()).hexdigest()
    
    def _is_duplicate_news(self, article: Dict) -> bool:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        
        Args:
            article: ‡∏Ç‡πà‡∏≤‡∏ß article dict
        
        Returns:
            True ‡∏ñ‡πâ‡∏≤‡∏ã‡πâ‡∏≥, False ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
        """
        news_hash = self._generate_news_hash(article)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏ô memory cache
        if news_hash in self.processed_news_hashes:
            return True
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏ô database (‡πÉ‡∏ä‡πâ collection post_yahoo)
        from utils.post_normalizer import get_collection_name
        collection_name = get_collection_name('yahoo')
        if db is not None and hasattr(db, collection_name) and getattr(db, collection_name) is not None:
            post_collection = getattr(db, collection_name)
            existing = post_collection.find_one({"newsHash": news_hash})
            if existing:
                return True
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ memory cache
        self.processed_news_hashes.add(news_hash)
        return False
    
    def _clean_old_data(self, symbol: str, days_to_keep: int = None, skip_reddit: bool = False):
        """
        ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô days_back ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å database
        
        Args:
            symbol: Stock symbol
            days_to_keep: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (default: self.days_back)
                          ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô None ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤ (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
            skip_reddit: ‡∏ñ‡πâ‡∏≤ True ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏•‡∏ö Reddit posts (Reddit bulk processor ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏á)
        """
        if db is None:
            return
        
        days_to_keep = days_to_keep or self.days_back
        
        # ‡∏ñ‡πâ‡∏≤ days_to_keep ‡πÄ‡∏õ‡πá‡∏ô None ‡∏´‡∏£‡∏∑‡∏≠ <= 0 ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤ (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
        if days_to_keep is None or (isinstance(days_to_keep, (int, float)) and days_to_keep <= 0):
            return  # ‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤ - ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ days_to_keep ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        if not isinstance(days_to_keep, (int, float)):
            return
        
        days_to_keep = int(days_to_keep)
        cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
        
        from utils.post_normalizer import get_collection_name
        
        try:
            # ‡∏•‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤ (‡πÉ‡∏ä‡πâ collection post_yahoo)
            collection_name = get_collection_name('yahoo')
            if db is not None and hasattr(db, collection_name) and getattr(db, collection_name) is not None:
                post_collection = getattr(db, collection_name)
                result = post_collection.delete_many({
                    "symbol": symbol.upper(),
                    "created_utc": {"$lt": cutoff_date.isoformat()}
                })
                # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á print ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ó‡∏±‡∏ö progress bar
                # print(f"  üóëÔ∏è Cleaned {result.deleted_count} old news articles for {symbol}")
            
            # ‚úÖ ‡πÑ‡∏°‡πà‡∏•‡∏ö Reddit posts ‡πÄ‡∏Å‡πà‡∏≤ - Reddit bulk processor ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏á
            # Reddit posts ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏î‡∏¢ Reddit bulk scheduler (‡∏ó‡∏∏‡∏Å 45 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
            # ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á
            # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö posts ‡πÄ‡∏Å‡πà‡∏≤ ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏´‡∏∏‡πâ‡∏ô)
            if not skip_reddit:
                collection_name = get_collection_name('reddit')
                if db is not None and hasattr(db, collection_name) and getattr(db, collection_name) is not None:
                    post_collection = getattr(db, collection_name)
                    result = post_collection.delete_many({
                        "keyword": symbol.upper(),
                        "created_utc": {"$lt": cutoff_date.isoformat()}
                    })
                    # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á print ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ó‡∏±‡∏ö progress bar
                    # print(f"  üóëÔ∏è Cleaned {result.deleted_count} old Reddit posts for {symbol}")
        except Exception as e:
            # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á print ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ó‡∏±‡∏ö progress bar
            # print(f"  ‚ö†Ô∏è Error cleaning old data for {symbol}: {e}")
            pass
    
    def _should_update_stock(self, symbol: str) -> bool:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        
        Args:
            symbol: Stock symbol
        
        Returns:
            True ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ß‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï, False ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á‡πÄ‡∏ß‡∏•‡∏≤
        """
        if db is None or not hasattr(db, 'stocks') or db.stocks is None:
            return True
        
        try:
            # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏ô‡∏µ‡πâ
            latest = db.stocks.find_one(
                {"symbol": symbol.upper()},
                sort=[("fetchedAt", -1)]
            )
            
            if not latest:
                return True  # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô update_interval ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
            fetched_at_str = latest.get('fetchedAt', '')
            if isinstance(fetched_at_str, str):
                fetched_at = datetime.fromisoformat(fetched_at_str.replace('Z', '+00:00'))
            else:
                fetched_at = fetched_at_str
            
            time_diff = datetime.utcnow() - fetched_at
            
            should_update = time_diff > timedelta(hours=self.update_interval_hours)
            return should_update
        except Exception as e:
            # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á print ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ó‡∏±‡∏ö progress bar
            # print(f"  ‚ö†Ô∏è Error checking update status for {symbol}: {e}")
            pass
            import traceback
            traceback.print_exc()
            return True  # ‡∏ñ‡πâ‡∏≤ error ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡πÉ‡∏´‡∏°‡πà
    
    def _deduplicate_news(self, articles: List[Dict]) -> List[Dict]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥‡∏≠‡∏≠‡∏Å
        
        Args:
            articles: List of news articles
        
        Returns:
            List of unique news articles
        """
        unique_articles = []
        seen_hashes = set()
        
        for article in articles:
            news_hash = self._generate_news_hash(article)
            
            if news_hash not in seen_hashes and not self._is_duplicate_news(article):
                article['newsHash'] = news_hash  # ‡πÄ‡∏û‡∏¥‡πà‡∏° hash ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô article
                unique_articles.append(article)
                seen_hashes.add(news_hash)
        
        return unique_articles
    
    async def process_single_stock_async(self, symbol: str) -> Optional[Dict]:
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏ö‡∏ö async
        
        Args:
            symbol: Stock symbol
        
        Returns:
            Aggregated stock data ‡∏´‡∏£‡∏∑‡∏≠ None
        """
        symbol_upper = symbol.upper()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        # ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏™‡∏°‡∏≠ (‡πÑ‡∏°‡πà skip ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß)
        should_update_stock = self._should_update_stock(symbol_upper)
        if not should_update_stock:
            # ‡∏î‡∏∂‡∏á stock info ‡∏à‡∏≤‡∏Å database ‡πÅ‡∏ó‡∏ô (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á‡πÉ‡∏´‡∏°‡πà)
            if db is not None and hasattr(db, 'stocks') and db.stocks is not None:
                cached_stock = db.stocks.find_one(
                    {"symbol": symbol_upper},
                    sort=[("fetchedAt", -1)]
                )
                if cached_stock:
                    stock_info = cached_stock.get('stockInfo', {})
                else:
                    stock_info = None
            else:
                stock_info = None
        else:
            stock_info = None  # ‡∏à‡∏∞‡∏î‡∏∂‡∏á‡πÉ‡∏´‡∏°‡πà
        
        try:
            news_count_before = 0
            if db is not None and hasattr(db, 'news') and db.news is not None:
                news_count_before = db.news.count_documents({"symbol": symbol_upper})
            # 1. Clean old data ‡∏Å‡πà‡∏≠‡∏ô‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
            # ‚úÖ ‡πÑ‡∏°‡πà‡∏•‡∏ö Reddit posts - Reddit bulk processor ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏á
            # Reddit posts ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏î‡∏¢ Reddit bulk scheduler (‡∏ó‡∏∏‡∏Å 45 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
            # ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á
            # ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ news articles ‡πÄ‡∏Å‡πà‡∏≤ (‡πÑ‡∏°‡πà‡∏•‡∏ö Reddit)
            self._clean_old_data(symbol_upper, skip_reddit=True)
            
            # 2-4. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö PARALLEL (‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô) - ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° 3 ‡πÄ‡∏ó‡πà‡∏≤
            # ‡∏î‡∏∂‡∏á stock_info, news, reddit ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏£‡∏≠‡∏ó‡∏µ‡∏•‡∏∞‡∏≠‡∏±‡∏ô
            async def fetch_stock_info_task():
                if not stock_info:
                    try:
                        return await self.async_fetcher.fetch_stock_info_async(symbol_upper)
                    except Exception:
                        return None
                return stock_info
            
            async def fetch_news_task():
                try:
                    # ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (500 ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô) - ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ß‡∏±‡∏ô
                    news_articles_raw = await self.async_fetcher.fetch_stock_news_async(symbol_upper, max_results=500)
                    if news_articles_raw:
                        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥ - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö database (‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ã‡πâ‡∏≥)
                        return self._deduplicate_news(news_articles_raw)
                    return []
                except Exception:
                    return []
            
            async def fetch_reddit_task():
                try:
                    # ‡∏ñ‡πâ‡∏≤ skip Reddit ‚Üí return empty list
                    if self.skip_reddit:
                        return []
                    
                    # ‚úÖ ‡πÉ‡∏ä‡πâ Reddit ‡∏à‡∏≤‡∏Å database ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (Reddit bulk processor ‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÉ‡∏´‡πâ)
                    # ‡πÑ‡∏°‡πà‡∏î‡∏∂‡∏á Reddit per-stock ‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ (‡∏ä‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)
                    from utils.post_normalizer import get_collection_name
                    collection_name = get_collection_name('reddit')
                    days_back_for_reddit = self.days_back if self.days_back is not None else 7
                    cutoff_time = datetime.utcnow() - timedelta(days=days_back_for_reddit)
                    
                    # ‡∏î‡∏∂‡∏á Reddit ‡∏à‡∏≤‡∏Å database (‡∏ó‡∏µ‡πà Reddit bulk processor ‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß)
                    if db is not None and hasattr(db, collection_name) and getattr(db, collection_name) is not None:
                        post_collection = getattr(db, collection_name)
                        
                        # ‡∏î‡∏∂‡∏á posts ‡∏ó‡∏µ‡πà‡∏°‡∏µ symbol ‡∏ô‡∏µ‡πâ‡πÉ‡∏ô symbols array
                        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á keyword (‡πÄ‡∏î‡∏¥‡∏°) ‡πÅ‡∏•‡∏∞ symbols (‡πÉ‡∏´‡∏°‡πà)
                        query = {
                            "$or": [
                                {"keyword": symbol_upper},
                                {"symbols": symbol_upper}
                            ],
                            "created_utc": {"$gte": cutoff_time.isoformat()}
                        }
                        
                        reddit_posts_cursor = post_collection.find(query).sort("created_utc", -1).limit(self.reddit_limit)
                        reddit_posts = list(reddit_posts_cursor)
                        
                        if reddit_posts:
                            return reddit_posts
                    
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô database ‚Üí return empty (Reddit bulk processor ‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÉ‡∏´‡πâ)
                    return []
                except Exception:
                    return []
            
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö parallel (‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô)
            stock_info_result, news_articles, reddit_posts = await asyncio.gather(
                fetch_stock_info_task(),
                fetch_news_task(),
                fetch_reddit_task(),
                return_exceptions=True
            )
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            if isinstance(stock_info_result, Exception) or not stock_info_result:
                stock_info = {
                    'symbol': symbol_upper,
                    'name': symbol_upper,
                    'currentPrice': 0,
                    'fetchedAt': datetime.utcnow().isoformat()
                }
            else:
                stock_info = stock_info_result
            
            if isinstance(news_articles, Exception):
                news_articles = []
            
            if isinstance(reddit_posts, Exception):
                reddit_posts = []
            
            # 5. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏à‡∏≤‡∏Å‡∏Ç‡πà‡∏≤‡∏ß (‡πÉ‡∏ä‡πâ time-weighted)
            sentiment = None
            if news_articles:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á items_with_dates ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö time-weighted analysis
                items_with_dates = []
                for a in news_articles:
                    title = a.get('title', '') or ''
                    selftext = a.get('selftext', '') or ''
                    full_content = a.get('full_content', '') or ''
                    # ‡πÉ‡∏ä‡πâ selftext ‡∏Å‡πà‡∏≠‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ full_content (‡∏à‡∏≥‡∏Å‡∏±‡∏î 500 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
                    content = selftext or (full_content[:500] if full_content else '')
                    text = f"{title} {content}".strip()
                    if text:  # ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                        items_with_dates.append({
                            'text': text,
                            'publishedAt': a.get('publishedAt') or a.get('providerPublishTime') or a.get('publish_date') or a.get('created_utc')
                        })
                
                if items_with_dates:
                    # ‡πÉ‡∏ä‡πâ time-weighted analysis ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤
                    sentiment = self.sentiment_analyzer.analyze_batch(
                        texts=[item['text'] for item in items_with_dates],
                        use_time_weighting=True,
                        items_with_dates=items_with_dates
                    )
                    # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á print ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ó‡∏±‡∏ö progress bar
                    # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ Redis cache - ‡∏•‡∏î memory usage
            
            # 6. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Reddit posts ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á‡πÉ‡∏´‡∏°‡πà)
            aggregated_data = {
                'redditData': {},
                'twitterData': {},
                'youtubeData': {},
                'trendsData': {}
            }
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Reddit posts
            if reddit_posts:
                try:
                    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment (‡πÉ‡∏ä‡πâ time-weighted)
                    items_with_dates = []
                    for p in reddit_posts:
                        text = f"{p.get('title', '')} {p.get('selftext', '')}"
                        if text.strip():
                            items_with_dates.append({
                                'text': text,
                                'publishedAt': p.get('publishedAt') or p.get('created_utc') or p.get('created_at')
                            })
                    
                    if items_with_dates:
                        # ‡πÉ‡∏ä‡πâ time-weighted analysis ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ post ‡πÉ‡∏´‡∏°‡πà‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤
                        reddit_sentiment = self.sentiment_analyzer.analyze_batch(
                            texts=[item['text'] for item in items_with_dates],
                            use_time_weighting=True,
                            items_with_dates=items_with_dates
                        )
                        aggregated_data['redditData'] = {
                            'posts': reddit_posts[:20],  # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà 20 posts
                            'sentiment': reddit_sentiment,
                            'mentionCount': len(reddit_posts)
                        }
                except Exception:
                    pass
            
            # 7. ‡πÉ‡∏ä‡πâ Enhanced Sentiment Aggregator (‡∏û‡∏£‡πâ‡∏≠‡∏° market confirmation)
            # ‡∏î‡∏∂‡∏á previous sentiment ‡∏à‡∏≤‡∏Å database
            previous_sentiment = None
            if db is not None and hasattr(db, 'stocks') and db.stocks is not None:
                previous_stock = db.stocks.find_one({"symbol": symbol_upper})
                if previous_stock and previous_stock.get('overallSentiment'):
                    previous_sentiment = previous_stock.get('overallSentiment')
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° news items ‡πÅ‡∏•‡∏∞ reddit items ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö enhanced aggregator
            news_items = []
            for article in news_articles:
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
                if 'sentiment' not in article:
                    title = article.get('title', '') or ''
                    selftext = article.get('selftext', '') or ''
                    text = f"{title} {selftext}".strip()
                    if text:
                        article['sentiment'] = self.sentiment_analyzer.analyze(text)
                article['source'] = article.get('source', 'yahoo_finance')
                news_items.append(article)
            
            reddit_items = []
            if reddit_posts:
                for post in reddit_posts:
                    # ‡πÉ‡∏ä‡πâ sentiment ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡πà‡∏≠ post)
                    if 'sentiment' not in post:
                        title = post.get('title', '') or ''
                        selftext = post.get('selftext', '') or ''
                        text = f"{title} {selftext}".strip()
                        if text:
                            post['sentiment'] = self.sentiment_analyzer.analyze(text)
                    post['source'] = 'reddit'
                    reddit_items.append(post)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì enhanced sentiment
            enhanced_sentiment = self.enhanced_sentiment_aggregator.aggregate_sentiment(
                news_items=news_items,
                reddit_items=reddit_items,
                stock_info=stock_info,
                previous_sentiment=previous_sentiment
            )
            
            # 8. ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            result = {
                'symbol': symbol_upper,
                'fetchedAt': datetime.utcnow().isoformat(),
                'stockInfo': stock_info,
                'newsData': {
                    'articles': news_articles[:50],  # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà 50 ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                    'sentiment': sentiment,  # ‡πÄ‡∏Å‡πá‡∏ö sentiment ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏ß‡∏¢
                    'articleCount': len(news_articles),
                    'source': 'yahoo_finance'
                },
                'redditData': aggregated_data.get('redditData', {}),
                'twitterData': aggregated_data.get('twitterData', {}),
                'youtubeData': aggregated_data.get('youtubeData', {}),
                'trendsData': aggregated_data.get('trendsData', {}),
                'overallSentiment': enhanced_sentiment  # ‡πÉ‡∏ä‡πâ enhanced sentiment
            }
            
            # 8. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á database
            if db is not None and hasattr(db, 'stocks') and db.stocks is not None:
                db.stocks.update_one(
                    {"symbol": symbol_upper},
                    {"$set": result},
                    upsert=True
                )
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å (‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏∏‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤) - ‡πÉ‡∏ä‡πâ collection post_yahoo
                from utils.post_normalizer import normalize_post, get_collection_name
                
                collection_name = get_collection_name('yahoo')
                if db is not None and hasattr(db, collection_name) and getattr(db, collection_name) is not None and news_articles:
                    post_collection = getattr(db, collection_name)
                    saved_count = 0
                    new_count = 0
                    for article in news_articles:
                        # Normalize post structure
                        normalized_article = normalize_post(article, 'yahoo', symbol_upper)
                        normalized_article['symbol'] = symbol_upper
                        normalized_article['fetched_at'] = datetime.utcnow().isoformat()
                        
                        try:
                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÉ‡∏ä‡πâ newsHash ‡∏´‡∏£‡∏∑‡∏≠ id)
                            news_hash = normalized_article.get('newsHash')
                            post_id = normalized_article.get('id')
                            
                            existing = None
                            if news_hash:
                                existing = post_collection.find_one({"newsHash": news_hash})
                            if not existing and post_id:
                                existing = post_collection.find_one({"id": post_id})
                            
                            is_new = existing is None
                            
                            # ‡πÉ‡∏ä‡πâ id ‡∏´‡∏£‡∏∑‡∏≠ newsHash ‡πÄ‡∏õ‡πá‡∏ô unique key
                            if news_hash:
                                post_collection.update_one(
                                    {"newsHash": news_hash},
                                    {"$set": normalized_article},
                                    upsert=True
                                )
                            elif post_id:
                                post_collection.update_one(
                                    {"id": post_id},
                                    {"$set": normalized_article},
                                    upsert=True
                                )
                            else:
                                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ id ‡∏´‡∏£‡∏∑‡∏≠ hash ‡πÉ‡∏´‡πâ insert ‡πÉ‡∏´‡∏°‡πà
                                post_collection.insert_one(normalized_article)
                            
                            saved_count += 1
                            if is_new:
                                new_count += 1
                        except Exception:
                            pass
                            continue
            
            return result
            
        except Exception as e:
            # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á error ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ progress bar ‡∏î‡∏π‡∏™‡∏∞‡∏≠‡∏≤‡∏î
            return None
    
    async def process_all_stocks_async(self, symbols: List[str], batch_size: int = 50) -> Dict[str, Dict]:
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏ö‡∏ö batch
        
        Args:
            symbols: List of stock symbols
            batch_size: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏∏‡πâ‡∏ô‡∏ï‡πà‡∏≠ batch
        
        Returns:
            Dictionary {symbol: stock_data}
        """
        from utils.post_normalizer import get_collection_name
        from utils.progress_bar import draw_progress_bar, reset_progress
        
        # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï progress bar ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà
        reset_progress()
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô database ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏° (‡πÉ‡∏ä‡πâ collection post_yahoo)
        total_news_before = 0
        collection_name = get_collection_name('yahoo')
        if db is not None and hasattr(db, collection_name) and getattr(db, collection_name) is not None:
            post_collection = getattr(db, collection_name)
            total_news_before = post_collection.count_documents({})
        
        all_results = {}
        total_stocks = len(symbols)
        start_time = time.time()
        
        # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á progress bar ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏° (0%) - ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î‡∏£‡∏±‡∏ô
        # ‡πÉ‡∏ä‡πâ print() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏ô Windows)
        print()  # ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏™‡∏î‡∏á progress bar
        import sys
        sys.stdout.flush()  # Force flush
        draw_progress_bar(0, total_stocks, bar_length=50, prefix="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πà‡∏≤‡∏ß", show_total=True)
        sys.stdout.flush()  # Force flush ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        
        for i in range(0, len(symbols), batch_size):
            batch = symbols[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(symbols) + batch_size - 1) // batch_size
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• batch ‡πÅ‡∏ö‡∏ö parallel
            tasks = [self.process_single_stock_async(symbol) for symbol in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            for symbol, result in zip(batch, batch_results):
                if not isinstance(result, Exception) and result:
                    all_results[symbol.upper()] = result
            
            # ‡πÅ‡∏™‡∏î‡∏á progress bar ‡∏´‡∏•‡∏±‡∏á batch ‡πÄ‡∏™‡∏£‡πá‡∏à (‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ)
            processed = len(all_results)
            draw_progress_bar(processed, total_stocks, bar_length=50, prefix="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πà‡∏≤‡∏ß", show_total=True)
            # Force flush ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
            import sys
            sys.stdout.flush()
            
            # ‡∏û‡∏±‡∏Å‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á batch (‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô 0.1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å)
            if i + batch_size < len(symbols):
                await asyncio.sleep(0.1)
        
        elapsed = time.time() - start_time
        
        # ‚úÖ ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏î‡πâ
        total_news_after = 0
        collection_name = get_collection_name('yahoo')
        if db is not None and hasattr(db, collection_name) and getattr(db, collection_name) is not None:
            post_collection = getattr(db, collection_name)
            total_news_after = post_collection.count_documents({})
        
        new_news = total_news_after - total_news_before
        
        # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
        print(f"\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤:")
        print(f"   üì∞ Yahoo News: {new_news:,} ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà (‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_news_after:,} ‡∏Ç‡πà‡∏≤‡∏ß)")
        print(f"   üìà Stocks: {len(all_results):,} ‡∏´‡∏∏‡πâ‡∏ô")
        print(f"   ‚è±Ô∏è  ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {elapsed/60:.1f} ‡∏ô‡∏≤‡∏ó‡∏µ")
        
        # Cleanup: ‡∏õ‡∏¥‡∏î aiohttp sessions
        try:
            # ‡∏õ‡∏¥‡∏î Yahoo Finance async fetcher session
            if hasattr(self.async_fetcher, '_yahoo_async_fetcher'):
                await self.async_fetcher._yahoo_async_fetcher.close()
            
            # Force garbage collection
            import gc
            gc.collect()
        except:
            pass
        
        return all_results
    
    def get_stock_from_database(self, symbol: str) -> Optional[Dict]:
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏à‡∏≤‡∏Å database (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á API)
        
        Args:
            symbol: Stock symbol
        
        Returns:
            Stock data ‡∏à‡∏≤‡∏Å database ‡∏´‡∏£‡∏∑‡∏≠ None
        """
        if not db or not db.stocks:
            return None
        
        try:
            result = db.stocks.find_one(
                {"symbol": symbol.upper()},
                sort=[("fetchedAt", -1)]
            )
            return result
        except Exception as e:
            # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á print ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ó‡∏±‡∏ö progress bar
            # print(f"‚ö†Ô∏è Error getting stock from database: {e}")
            pass
            return None
    
    def get_all_stocks_from_database(self, limit: int = None) -> List[Dict]:
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å database
        
        Args:
            limit: ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô (None = ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
        
        Returns:
            List of stock data
        """
        if not db or not db.stocks:
            return []
        
        try:
            query = db.stocks.find().sort("fetchedAt", -1)
            if limit:
                query = query.limit(limit)
            return list(query)
        except Exception as e:
            # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á print ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ó‡∏±‡∏ö progress bar
            # print(f"‚ö†Ô∏è Error getting all stocks from database: {e}")
            pass
            return []


# Global instance
batch_processor = BatchDataProcessor(days_back=7, update_interval_hours=2)

